from sklearn.feature_extraction.text import TfidfVectorizer
# list of text documents
''' 
the: 1
fox: 2
dog: 3
jumped: 4
over: 5
'''
text = ["265 168 168 265 168 168 168 265 168 265 168 168 168 168 168 168 168 168 102 168 265 265 168 168 168 265 265 168 168 168 168 265 102 265 265 168 168 168 168 265 168 168 265 102 168 168 168 168 168 168 168 168 168 265 168 168 168 168 168 168 265 168 168 168 168 265 265 265 168 265 168 265 168 168 168 168 168 168 168 265 168 265 168 168 168 168 168 102 168 265 265 168 168 168 168 265 168 168 265 265 168 168 168 168 265 265 168 168 168 265 168 265 168 168 265 265 265 265 265 168 168 265 168 265 168 168 168 265 168 168 168 168 265 168 265 168 168 168 168 168 168 168 168 265 168 168 168 168 168 168 168 265 168 168 168 168 168 168 265 102 168 265 168 168 265 168 265 265 265 265 168 265 265 168 168 168 168 168 168 168 102 265 168 265 168 265 168 168 168 168 168 168 168 265 265 168 168 265 265 265 168 102 168 265 168 168 168 168 168 265 168 168 168 102 168 168 168 168 168 168 168 168 265 265 168 265 168 168 265 265 265 265 168 168 168 168 168 265 168 168 168 168 168 168 168 168 168 168 168 168 168 168 168 168 168 265 265 102 168 265 168 168 265 168 168 168 168 168 168 102 168 168 168 168 265 168 168 168 168"]
# create the transform
vectorizer = TfidfVectorizer(norm=None, ngram_range=(5,5))
# tokenize and build vocab
vectorizer.fit(text)
# summarize
print(vectorizer.vocabulary_)
print(len(vectorizer.vocabulary_))
print("IDF values: " + str(vectorizer.idf_))
# encode document
vector = vectorizer.transform([text[0]])
# summarize encoded vector
# print(vector.shape)
print("Transformed vector: ")
print(vector.toarray())